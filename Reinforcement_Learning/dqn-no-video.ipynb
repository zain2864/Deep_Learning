{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you fill your name and NetID below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "NET_ID = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Assignment 3: Reinforcement Learning\n",
    "\n",
    "In this assignment, you will implement a few reinforcement learning algorithms:\n",
    "\n",
    "1. Deep Q-learning (DQN)\n",
    "2. Policy Gradients (PG)\n",
    "3. Actor-Critic (AC)\n",
    "\n",
    "As in the previous assignments, you will see code blocks that look like this:\n",
    "```python\n",
    "###############################################################################\n",
    "# TODO: Create a variable x with value 3.7\n",
    "###############################################################################\n",
    "pass\n",
    "# END OF YOUR CODE\n",
    "```\n",
    "\n",
    "You should replace the `pass` statement with your own code and leave the blocks intact, like this:\n",
    "```python\n",
    "###############################################################################\n",
    "# TODO: Create a variable x with value 3.7\n",
    "###############################################################################\n",
    "x = 3.7\n",
    "# END OF YOUR CODE\n",
    "```\n",
    "\n",
    "Also, please remember:\n",
    "- Do not write or modify any code outside of code blocks\n",
    "- Do not add or delete any cells from the notebook. You may add new cells to perform scatch work, but delete them before submitting.\n",
    "- Run all cells before submitting. You will only get credit for code that has been run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Environment\n",
    "\n",
    "We'll first introduce a test environment that we can use to test our algorithms quickly. Your models should run on this environment in no more than a few minutes. Consider the following environment:\n",
    "\n",
    "- **4 states**: 0, 1, 2, 3\n",
    "- **5 actions**: 0, 1, 2, 3, 4. Action $0 \\le i \\le 3$ goes straight to state $i$. Action 4 makes the agent stay in the same state.\n",
    "- **Rewards**: Going to state $i$ from states 0, 1, and 3 gives a reward $R(i)$, where $R(0)=0.1$, $R(1)=-0.2$, $R(2)=0$, $R(3)=-0.1$. If we start in state 2, then the rewards defined above are multiplied by -20.\n",
    "- **Episode length**: One episode lasts 5 timesteps (for a total of 5 actions) and always starts in state 0 (no rewards at the initial state).\n",
    "\n",
    "For example, one episode may have the following actions: 1, 2, 4, 3, 0. This would result in the following states: 0 (initial), 1, 2, 2, 3, 0. And the following rewards: -0.2, 0, 0, -20*-0.1=2, 0.1.\n",
    "\n",
    "Assuming the discount rate $\\gamma=1$, what is the maximum sum of rewards that can be achieved in a single trajectory in the test environment? Assign your answer to the variable `max_reward`. Briefly explain in a comment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c1fcee8f92efc9279ed868dd524e3d3",
     "grade": false,
     "grade_id": "cell-70ea71a1c6566be1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "max_reward = None\n",
    "##############################################################################\n",
    "# TODO: Set max_reward to the maximum sum of rewards that can be achieved\n",
    "# in a single trajectory in the above test environment.\n",
    "# Briefly explain in a python comment.\n",
    "##############################################################################\n",
    "# Replace \"pass\" statement with your code\n",
    "pass\n",
    "# END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First let's install and import some libraries that will be useful in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym\n",
    "!pip install autorom\n",
    "!AutoROM -y\n",
    "!pip install --upgrade \"gym[atari]==0.21.0\"\n",
    "!pip uninstall ale-py --yes\n",
    "!pip install ale-py\n",
    "!pip uninstall box2d-py --yes\n",
    "!pip install box2d-py\n",
    "!pip uninstall pyvirtualdisplay --yes\n",
    "!pip install pyvirtualdisplay\n",
    "\n",
    "!apt-get install -y xvfb python-opengl ffmpeg\n",
    "\n",
    "import gym\n",
    "import ale_py\n",
    "\n",
    "# make sure the gym version is 0.21.0\n",
    "print('gym:', gym.__version__)\n",
    "print('ale_py:', ale_py.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "# from gym.wrappers import Monitor\n",
    "from gym.wrappers import RecordVideo, RecordEpisodeStatistics\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import random\n",
    "import sys, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "from IPython import display as ipythondisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup display for showing videos\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('Good to go!')\n",
    "else:\n",
    "    print('Please set GPU via Edit -> Notebook Settings.')\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using OpenAI Gym environments for our experiments. At a high level, OpenAI Gym provides environments to test your reinforcement learning agents using a shared interface. Agents choose actions to step through the environment and obtain rewards.\n",
    "\n",
    "Given an environment `env`, `env.reset()` is first called to return an initial observation from the environment. The agent can then use the observation to choose an action. Then, `env.step(action)` can be called which returns the observation, reward, done boolean, and diagnostic info for that action. For example, in the below snippet, we execute a random action on the CartPole environment. Note that the observation is ignored here, but in practice, we would use the observation to generate an action (ie. use a policy based on our model).\n",
    "\n",
    "```python\n",
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        print(observation)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "env.close()\n",
    "```\n",
    "\n",
    "See following documentation for more info: https://gym.openai.com/docs/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create our test environment from above. Note that the observation space consists of random integers of a given shape mapped to each underlying state. Our agent will learn how to map these observations to the underlying states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionSpace(object):\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "\n",
    "    def seed(self, seed):\n",
    "        #no-op\n",
    "        pass\n",
    "\n",
    "    def sample(self):\n",
    "        return np.random.randint(0, self.n)\n",
    "\n",
    "\n",
    "class ObservationSpace(object):\n",
    "    def __init__(self, shape):\n",
    "        self.shape = shape\n",
    "        self.state_0 = np.random.randint(0, 50, shape, dtype=np.uint16)\n",
    "        self.state_1 = np.random.randint(100, 150, shape, dtype=np.uint16)\n",
    "        self.state_2 = np.random.randint(200, 250, shape, dtype=np.uint16)\n",
    "        self.state_3 = np.random.randint(300, 350, shape, dtype=np.uint16)\n",
    "        self.states = [self.state_0, self.state_1, self.state_2, self.state_3]\n",
    "\n",
    "\n",
    "class EnvTest(object):\n",
    "    \"\"\"\n",
    "    Adapted from Igor Gitman, CMU / Karan Goel\n",
    "    Modified\n",
    "    \"\"\"\n",
    "    def __init__(self, shape=(80, 80, 3)):\n",
    "        #4 states\n",
    "        self.rewards = [0.1, -0.2, 0.0, -0.1]\n",
    "        self.cur_state = 0\n",
    "        self.num_iters = 0\n",
    "        self.was_in_second = False\n",
    "        self.action_space = ActionSpace(5)\n",
    "        self.observation_space = ObservationSpace(shape)\n",
    "\n",
    "    def seed(self, seed):\n",
    "        #no-op\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        self.cur_state = 0\n",
    "        self.num_iters = 0\n",
    "        self.was_in_second = False\n",
    "        return self.observation_space.states[self.cur_state]\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        assert(0 <= action <= 4)\n",
    "        self.num_iters += 1\n",
    "        if action < 4:\n",
    "            self.cur_state = action\n",
    "        reward = self.rewards[self.cur_state]\n",
    "        if self.was_in_second is True:\n",
    "            reward *= -20\n",
    "        if self.cur_state == 2:\n",
    "            self.was_in_second = True\n",
    "        else:\n",
    "            self.was_in_second = False\n",
    "        return self.observation_space.states[self.cur_state], reward, self.num_iters >= 5, {'ale.lives':0}\n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        print(self.cur_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Q-learning, we try estimate the optimal action-value function $Q^*(s,a)$ with a function $Q(s,a)$ by executing the following update:\n",
    "\n",
    "$$ Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big{(} r + \\gamma \\max_{a' \\in A} Q(s', a') - Q(s,a)\\Big{)} $$\n",
    "\n",
    "where $(s, a, r, s')$ is an experience sample, $A$ is the action space, $\\alpha > 0$ is the learning rate, and $\\gamma \\in [0, 1)$ is the discount factor.\n",
    "\n",
    "In cases where the state space is really large (eg. images), we approximate $Q(s,a)$ by $Q_\\theta(s,a)$ where $\\theta$ are parameters of a function (eg. neural network weights and biases) that are shared across all $(s,a)$. In this case, we aim to mimimize the following loss:\n",
    "\n",
    "$$ \\mathcal{L(\\theta)} = \\mathbb{E}_{s,a,r,s'\\sim \\mathcal{D}}\\Big{[} \\Big{(} r + \\gamma \\max_{a' \\in A} Q_\\theta(s', a') - Q_\\theta(s,a) \\Big{)}^2 \\Big{]} $$\n",
    "\n",
    "This looks like a supervised learning objective where we try to minimize the error between *targets* and *predictions*:\n",
    "\n",
    "*targets:* $r + \\gamma \\max_{a' \\in A} Q_\\theta(s', a')$\n",
    "\n",
    "*predictions:* $Q_\\theta(s,a)$\n",
    "\n",
    "However, there are a few critical differences from supervised learning that are problematic and make it difficult to train with this objective alone. First, the targets also depend on $\\theta$ so they are not fixed, as they are in supervised learning. This makes learning difficult since the function we are trying to learn constantly changes. Second, since the experience samples $(s, a, r, s')$ are obtained from running the agent in the environment, they are highly correlated, non-stationary, and not iid, which may lead to unstable training. \n",
    "\n",
    "Deepmind's seminal DQN paper (https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf, https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) proposes using **target networks** and **replay memory** to alleviate these issues.\n",
    "\n",
    "**Target Networks**: To solve the problem of the moving target, the DQN paper introduces a second network that is identical to the first network, but with weights that are frozen for a certain number of timesteps. If we denote this network by $Q_{\\theta^T}$, the objective now becomes:\n",
    "\n",
    "$$ \\mathcal{L(\\theta)} = \\mathbb{E}_{s,a,r,s'\\sim \\mathcal{D}}\\Big{[} \\Big{(} r + \\gamma \\max_{a' \\in A} Q_{\\theta^T}(s', a') - Q_\\theta(s,a) \\Big{)}^2 \\Big{]} $$\n",
    "\n",
    "The only difference is that the parameters of the target network are $\\theta^T$ instead of $\\theta$. We update $\\theta^T$ by copying the weights of $\\theta$ after every $C$ timesteps, where $C$ is a hyperparameter.\n",
    "\n",
    "**Replay Memory**: DQN also introduces a replay memory which is a fixed sized dataset that consists of $(s, a, r, s')$ tuples. The replay memory is used to store the $k$ most recent experiences where $k$ is another hyperparameter. The agent stores experience tuples into the replay memory as it interacts with the environment. We then sample a random minibatch from the replay memory when we train the $Q_\\theta$ network. Since the samples are randomly sampled, they are less correlated than if we used the agent's experience directly.\n",
    "\n",
    "In order to explore the environment, we use an $\\epsilon$-**Greedy Exploration Strategy**. This means that with probability $\\epsilon$, an action is chosen uniformly at random from $A$, the action space. With probability $1-\\epsilon$, the greedy action (ie. $\\text{argmax}_{a \\in A}Q_{\\theta}(s,a)$) is chosen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Schedule\n",
    "\n",
    "We use a linear schedule for our learning rate. Given a `value_begin`, a `value_end`, and `nsteps`, the learning rate starts at `value_begin` and decreases linearly to `value_end` over `n_steps`. Implement this linear schedule below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "405386891e16db59f8432cc4d3fc5450",
     "grade": false,
     "grade_id": "cell-903e12b1f2357c95",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class LinearSchedule(object):\n",
    "    def __init__(self, value_begin, value_end, nsteps):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            value_begin: initial value\n",
    "            value_end: end value\n",
    "            nsteps: number of steps between the two values\n",
    "        \"\"\"\n",
    "        self.value        = value_begin\n",
    "        self.value_begin  = value_begin\n",
    "        self.value_end    = value_end\n",
    "        self.nsteps       = nsteps\n",
    "\n",
    "\n",
    "    def update(self, t):\n",
    "        \"\"\"\n",
    "        Updates value\n",
    "\n",
    "        Args:\n",
    "            t: int\n",
    "                frame number\n",
    "        \"\"\"\n",
    "        ##############################################################################\n",
    "        # TODO: modify self.value such that\n",
    "        #       it is a linear interpolation from self.value_begin to\n",
    "        #       self.value_end as t goes from 0 to self.nsteps\n",
    "        #       For t > self.nsteps self.value remains constant\n",
    "        ##############################################################################\n",
    "        # Replace \"pass\" statement with your code\n",
    "        pass\n",
    "        # END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration Schedule\n",
    "\n",
    "We use a similar linear schedule for our exploration probability $\\epsilon$. Generally, we'll start with an $\\epsilon$ of 1 and decrease to a small number (eg. 0.1). This is because in the early parts of training, our learned policy probably isn't very good, so we take many random actions early on to explore the state space. As the learned policy gets better, we decrease $\\epsilon$, but always keep it greater than 0.\n",
    "\n",
    "The `LinearExploration` class below is a subclass of the `LinearSchedule` you just implemented. Implement the function `get_action` that returns a random action with probability `self.value` and the passed in `best_action` with probability `1 - self.value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6090e0c53949c7c9d5091db28a75d1fa",
     "grade": false,
     "grade_id": "cell-1e6400e75b5bb2f7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class LinearExploration(LinearSchedule):\n",
    "    def __init__(self, env, value_begin, value_end, nsteps):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            env: gym environment\n",
    "            value_begin: float\n",
    "                initial exploration rate\n",
    "            value_end: float\n",
    "                final exploration rate\n",
    "            nsteps: int\n",
    "                number of steps taken to linearly decay value_begin to value_end\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        super(LinearExploration, self).__init__(value_begin, value_end, nsteps)\n",
    "\n",
    "\n",
    "    def get_action(self, best_action):\n",
    "        \"\"\"\n",
    "        Returns a random action with prob value, otherwise returns the best_action\n",
    "\n",
    "        Args:\n",
    "            best_action: int\n",
    "                best action according some policy\n",
    "        Returns:\n",
    "            an action\n",
    "        \"\"\"\n",
    "        ##############################################################################\n",
    "        # TODO: with probability self.value, return a random action\n",
    "        #       else, return best_action.\n",
    "        #       - you can access the environment via self.env.\n",
    "        #       - you may use env.action_space.sample() to generate a random action\n",
    "        #       - use np.random.random() to generate a random number\n",
    "        ##############################################################################\n",
    "        # Replace \"pass\" statement with your code\n",
    "        pass\n",
    "        # END OF YOUR CODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = EnvTest((5, 5, 1))\n",
    "exp_strat = LinearExploration(env, 1, 0, 10)\n",
    "\n",
    "found_diff = False\n",
    "for i in range(10):\n",
    "    rnd_act = exp_strat.get_action(0)\n",
    "    if rnd_act != 0 and rnd_act is not None:\n",
    "        found_diff = True\n",
    "assert found_diff, \"Test 1 failed.\"\n",
    "\n",
    "env = EnvTest((5, 5, 1))\n",
    "exp_strat = LinearExploration(env, 1, 0, 10)\n",
    "exp_strat.update(5)\n",
    "assert exp_strat.value == 0.5, \"Test 2 failed\"\n",
    "\n",
    "\n",
    "env = EnvTest((5, 5, 1))\n",
    "exp_strat = LinearExploration(env, 1, 0.5, 10)\n",
    "exp_strat.update(20)\n",
    "assert exp_strat.value == 0.5, \"Test 3 failed\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q Network\n",
    "\n",
    "Let's now implement the $Q_\\theta$ network. After preprocessing the image (we've implemented this for you -- we make sure the input is the correct shape and permute the dimensions to (B, C, H, W)), we will use the following network:\n",
    "\n",
    "- Conv2d with 32 output channels, 8x8 kernel size, stride 4\n",
    "- ReLU\n",
    "- Conv2d with 64 output channels, 4x4 kernel size, stride 2\n",
    "- ReLU\n",
    "- Conv2d with 64 output channels, 3x3 kernel size, stride 1\n",
    "- ReLU\n",
    "- Flatten\n",
    "- Linear layer with 512 output units (you will need to figure out the input size)\n",
    "- ReLU\n",
    "- Linear layer with num_actions output units\n",
    "\n",
    "The input image to this network will be resized to 80x80. You will need to figure out what the input size will be in the first Linear layer. Notice that the output of this network has size num_actions, so to get the Q value of state $(s,a)$, we input $s$ to the network and look at the $a$-th entry of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "37c0a8fbd583434c6a0683fb32c22108",
     "grade": false,
     "grade_id": "cell-da0f56fd8d33e442",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class PreprocessImage(nn.Module):\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) < 4:\n",
    "            x = x[None, ...]\n",
    "        x = x.permute(0, 3, 1, 2).contiguous()\n",
    "        return x\n",
    "\n",
    "def create_q_network(ob_dim, num_actions, frame_history_len):\n",
    "    H, W, C = ob_dim\n",
    "    input_channels = C*frame_history_len\n",
    "    return nn.Sequential(\n",
    "        PreprocessImage(),\n",
    "        ##############################################################################\n",
    "        # TODO: Implement the network described above.\n",
    "        ##############################################################################\n",
    "        # Replace \"pass\" statement with your code\n",
    "        pass\n",
    "        # END OF YOUR CODE\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Memory\n",
    "\n",
    "We provide an optimized implementation of the replay memory for you. You do not have to implement anything here, but you may want to read through the code to understand what it is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_n_unique(sampling_f, n):\n",
    "    \"\"\"Helper function. Given a function `sampling_f` that returns\n",
    "    comparable objects, sample n such unique objects.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    while len(res) < n:\n",
    "        candidate = sampling_f()\n",
    "        if candidate not in res:\n",
    "            res.append(candidate)\n",
    "    return res\n",
    "  \n",
    "class ReplayBuffer(object):\n",
    "    \"\"\"\n",
    "    Taken from Berkeley's Assignment\n",
    "    \"\"\"\n",
    "    def __init__(self, size, frame_history_len):\n",
    "        \"\"\"This is a memory efficient implementation of the replay buffer.\n",
    "\n",
    "        The sepecific memory optimizations use here are:\n",
    "            - only store each frame once rather than k times\n",
    "              even if every observation normally consists of k last frames\n",
    "            - store frames as np.uint8 (actually it is most time-performance\n",
    "              to cast them back to float32 on GPU to minimize memory transfer\n",
    "              time)\n",
    "            - store frame_t and frame_(t+1) in the same buffer.\n",
    "\n",
    "        For the tipical use case in Atari Deep RL buffer with 1M frames the total\n",
    "        memory footprint of this buffer is 10^6 * 84 * 84 bytes ~= 7 gigabytes\n",
    "\n",
    "        Warning! Assumes that returning frame of zeros at the beginning\n",
    "        of the episode, when there is less frames than `frame_history_len`,\n",
    "        is acceptable.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "        frame_history_len: int\n",
    "            Number of memories to be retried for each observation.\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.frame_history_len = frame_history_len\n",
    "\n",
    "        self.next_idx      = 0\n",
    "        self.num_in_buffer = 0\n",
    "\n",
    "        self.obs      = None\n",
    "        self.action   = None\n",
    "        self.reward   = None\n",
    "        self.done     = None\n",
    "\n",
    "    def can_sample(self, batch_size):\n",
    "        \"\"\"Returns true if `batch_size` different transitions can be sampled from the buffer.\"\"\"\n",
    "        return batch_size + 1 <= self.num_in_buffer\n",
    "\n",
    "    def _encode_sample(self, idxes):\n",
    "        obs_batch      = np.concatenate([self._encode_observation(idx)[None] for idx in idxes], 0)\n",
    "        act_batch      = self.action[idxes]\n",
    "        rew_batch      = self.reward[idxes]\n",
    "        next_obs_batch = np.concatenate([self._encode_observation(idx + 1)[None] for idx in idxes], 0)\n",
    "        done_mask      = np.array([1.0 if self.done[idx] else 0.0 for idx in idxes], dtype=np.float32)\n",
    "\n",
    "        return obs_batch, act_batch, rew_batch, next_obs_batch, done_mask\n",
    "\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample `batch_size` different transitions.\n",
    "\n",
    "        i-th sample transition is the following:\n",
    "\n",
    "        when observing `obs_batch[i]`, action `act_batch[i]` was taken,\n",
    "        after which reward `rew_batch[i]` was received and subsequent\n",
    "        observation  next_obs_batch[i] was observed, unless the epsiode\n",
    "        was done which is represented by `done_mask[i]` which is equal\n",
    "        to 1 if episode has ended as a result of that action.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            Array of shape\n",
    "            (batch_size, img_h, img_w, img_c * frame_history_len)\n",
    "            and dtype np.uint8\n",
    "        act_batch: np.array\n",
    "            Array of shape (batch_size,) and dtype np.int32\n",
    "        rew_batch: np.array\n",
    "            Array of shape (batch_size,) and dtype np.float32\n",
    "        next_obs_batch: np.array\n",
    "            Array of shape\n",
    "            (batch_size, img_h, img_w, img_c * frame_history_len)\n",
    "            and dtype np.uint8\n",
    "        done_mask: np.array\n",
    "            Array of shape (batch_size,) and dtype np.float32\n",
    "        \"\"\"\n",
    "        assert self.can_sample(batch_size)\n",
    "        idxes = sample_n_unique(lambda: random.randint(0, self.num_in_buffer - 2), batch_size)\n",
    "        return self._encode_sample(idxes)\n",
    "      \n",
    "    def encode_recent_observation(self):\n",
    "        \"\"\"Return the most recent `frame_history_len` frames.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        observation: np.array\n",
    "            Array of shape (img_h, img_w, img_c * frame_history_len)\n",
    "            and dtype np.uint8, where observation[:, :, i*img_c:(i+1)*img_c]\n",
    "            encodes frame at time `t - frame_history_len + i`\n",
    "        \"\"\"\n",
    "        assert self.num_in_buffer > 0\n",
    "        return self._encode_observation((self.next_idx - 1) % self.size)\n",
    "\n",
    "    def _encode_observation(self, idx):\n",
    "        end_idx   = idx + 1 # make noninclusive\n",
    "        start_idx = end_idx - self.frame_history_len\n",
    "        # this checks if we are using low-dimensional observations, such as RAM\n",
    "        # state, in which case we just directly return the latest RAM.\n",
    "        # if len(self.obs.shape) <= 2:\n",
    "        #     return self.obs[end_idx-1]\n",
    "        # if there weren't enough frames ever in the buffer for context\n",
    "        if start_idx < 0 and self.num_in_buffer != self.size:\n",
    "            start_idx = 0\n",
    "        for idx in range(start_idx, end_idx - 1):\n",
    "            if self.done[idx % self.size]:\n",
    "                start_idx = idx + 1\n",
    "        missing_context = self.frame_history_len - (end_idx - start_idx)\n",
    "        # if zero padding is needed for missing context\n",
    "        # or we are on the boundry of the buffer\n",
    "        if start_idx < 0 or missing_context > 0:\n",
    "            frames = [np.zeros_like(self.obs[0]) for _ in range(missing_context)]\n",
    "            for idx in range(start_idx, end_idx):\n",
    "                frames.append(self.obs[idx % self.size])\n",
    "            return np.concatenate(frames, 2)\n",
    "        else:\n",
    "            # this optimization has potential to saves about 30% compute time \\o/\n",
    "            img_h, img_w = self.obs.shape[1], self.obs.shape[2]\n",
    "            return self.obs[start_idx:end_idx].transpose(1, 2, 0, 3).reshape(img_h, img_w, -1)\n",
    "\n",
    "    def store_frame(self, frame):\n",
    "        \"\"\"Store a single frame in the buffer at the next available index, overwriting\n",
    "        old frames if necessary.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        frame: np.array\n",
    "            Array of shape (img_h, img_w, img_c) and dtype np.uint8\n",
    "            the frame to be stored\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        idx: int\n",
    "            Index at which the frame is stored. To be used for `store_effect` later.\n",
    "        \"\"\"\n",
    "        if self.obs is None:\n",
    "            self.obs      = np.empty([self.size] + list(frame.shape), dtype=np.uint8)\n",
    "            self.action   = np.empty([self.size],                     dtype=np.int32)\n",
    "            self.reward   = np.empty([self.size],                     dtype=np.float32)\n",
    "            self.done     = np.empty([self.size],                     dtype=np.bool)\n",
    "        self.obs[self.next_idx] = frame\n",
    "\n",
    "        ret = self.next_idx\n",
    "        self.next_idx = (self.next_idx + 1) % self.size\n",
    "        self.num_in_buffer = min(self.size, self.num_in_buffer + 1)\n",
    "\n",
    "        return ret\n",
    "      \n",
    "    def store_effect(self, idx, action, reward, done):\n",
    "        \"\"\"Store effects of action taken after obeserving frame stored\n",
    "        at index idx. The reason `store_frame` and `store_effect` is broken\n",
    "        up into two functions is so that once can call `encode_recent_observation`\n",
    "        in between.\n",
    "\n",
    "        Paramters\n",
    "        ---------\n",
    "        idx: int\n",
    "            Index in buffer of recently observed frame (returned by `store_frame`).\n",
    "        action: int\n",
    "            Action that was performed upon observing this frame.\n",
    "        reward: float\n",
    "            Reward that was received when the actions was performed.\n",
    "        done: bool\n",
    "            True if episode was finished after performing that action.\n",
    "        \"\"\"\n",
    "        self.action[idx] = action\n",
    "        self.reward[idx] = reward\n",
    "        self.done[idx]   = done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN\n",
    "\n",
    "Now let's implement the main DQN class. Read the code to understand what it is doing (start with `train`). Then you will need to add your implementation to the following methods:\n",
    "\n",
    "- `get_best_action`\n",
    "- `update_target_params`\n",
    "- `update_step`\n",
    "\n",
    "Read the `# TODO` comments carefully for instructions of the parts you need to implement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2001e2476bc1df6b155c71eaa201fcf8",
     "grade": false,
     "grade_id": "cell-dde0c9bcb0a16c52",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, env, config):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            env: gym environment\n",
    "            config: dictionary\n",
    "                configuration parameters specific to the environment\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.config = config\n",
    "        self.gamma = config['gamma']\n",
    "\n",
    "        obj_dim = self.env.observation_space.shape\n",
    "        num_actions = self.env.action_space.n\n",
    "        q_net_func = config['q_net_func']\n",
    "        self.q_net = q_net_func(obj_dim, num_actions, self.config['frame_history_len']).to(config['device'])\n",
    "        self.q_net_target = q_net_func(obj_dim, num_actions, self.config['frame_history_len']).to(config['device'])\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=self.config['lr_begin'])\n",
    "\n",
    "        # AKA Huber loss. This loss is similar to the L2 loss at values around 0\n",
    "        # For larger values, it behaves like the L1 loss. This works well for the environments\n",
    "        # we are testing and a similar loss is used in the DQN paper.\n",
    "        self.loss = nn.SmoothL1Loss()\n",
    "        \n",
    "    def get_best_action(self, q_input):\n",
    "        \"\"\"\n",
    "        Retrieves the best action and the q_values for a given q_input\n",
    "\n",
    "        Args:\n",
    "            q_input: input the q_net representing the current state. Multiple\n",
    "            frames may be stacked together depending on frame_history_len.\n",
    "            Dimensions are (H, W, C) where C is the number of channels * frame_history_len\n",
    "        Returns:\n",
    "            best_action\n",
    "            q_values\n",
    "        \"\"\"\n",
    "        q_input = torch.from_numpy(q_input).float().to(self.config['device'])\n",
    "        ##############################################################################\n",
    "        # TODO: First use q_net to obtain the q_values for the given input.\n",
    "        # Then, use torch.argmax to obtain the best_action given the q_values.\n",
    "        # Return the best_action and the q_values\n",
    "        ##############################################################################\n",
    "        # Replace \"pass\" statement with your code\n",
    "        pass\n",
    "        # END OF YOUR CODE\n",
    "\n",
    "    def update_target_params(self):\n",
    "        \"\"\"\n",
    "        Update q_net_target parameters by copying parameters from q_net\n",
    "        \"\"\"\n",
    "        ##############################################################################\n",
    "        # TODO: For each parameter in q_net, copy the param.data to q_net_target.\n",
    "        # You can use self.q_net.parameters() to access the parameters in the network.\n",
    "        # To set the value of a parameter, you can use `parameter.data.copy_()`\n",
    "        ##############################################################################\n",
    "        # Replace \"pass\" statement with your code\n",
    "        pass\n",
    "        # END OF YOUR CODE\n",
    "            \n",
    "    def schedule_step(self, t, lr_schedule, exp_schedule):\n",
    "        \"\"\"\n",
    "        Update the learning rate end exploration epsilon\n",
    "        \"\"\"\n",
    "        lr_schedule.update(t)\n",
    "        for pg in self.optimizer.param_groups:\n",
    "            pg['lr'] = lr_schedule.value\n",
    "\n",
    "        exp_schedule.update(t)\n",
    "        \n",
    "    def train(self, lr_schedule, exp_schedule):\n",
    "        print('Starting...')\n",
    "        replay_buffer = ReplayBuffer(self.config['buffer_size'], self.config['frame_history_len'])\n",
    "        rewards = []\n",
    "        eval_rewards_list = []\n",
    "        max_q_values = deque(maxlen=1000)\n",
    "        q_values = deque(maxlen=1000)\n",
    "        n = 0\n",
    "        t = 0\n",
    "        last_eval = 0\n",
    "        last_video = 0\n",
    "        while t < self.config['nsteps_train']:\n",
    "            n += 1\n",
    "            total_reward = 0\n",
    "            total_loss = 0\n",
    "            state = self.env.reset()\n",
    "\n",
    "            # update learning rate and exploration schedule\n",
    "            self.schedule_step(t, lr_schedule, exp_schedule)\n",
    "\n",
    "            # Run through an episode of the game\n",
    "            while True:\n",
    "                t += 1\n",
    "                last_eval += 1\n",
    "                last_video += 1\n",
    "                # store latest frame\n",
    "                idx = replay_buffer.store_frame(state)\n",
    "                # retrieve last frame_history_len (4) frames\n",
    "                # in DQN, we stack the last 4 frames as one observation\n",
    "                q_input = replay_buffer.encode_recent_observation()\n",
    "\n",
    "                # chose action according to current Q and exploration\n",
    "                best_action, q_values = self.get_best_action(q_input)\n",
    "                action = exp_schedule.get_action(best_action)\n",
    "\n",
    "                # store q values for debugging\n",
    "                q_values = q_values.cpu().numpy()\n",
    "                max_q_values.append(max(q_values))\n",
    "                q_values += list(q_values)\n",
    "\n",
    "                # perform action in env\n",
    "                new_state, reward, done, info = self.env.step(action)\n",
    "\n",
    "                # store the transition\n",
    "                replay_buffer.store_effect(idx, action, reward, done)\n",
    "                state = new_state\n",
    "\n",
    "                # perform a training step\n",
    "                loss = self.train_step(t, replay_buffer, lr_schedule.value)\n",
    "                total_loss += loss\n",
    "\n",
    "                # count reward\n",
    "                total_reward += reward\n",
    "                if done or t >= self.config['nsteps_train']:\n",
    "                    break\n",
    "\n",
    "            # updates to perform at the end of an episode\n",
    "            rewards.append(total_reward)\n",
    "\n",
    "            if (t > self.config['learning_start']) and (last_eval > self.config['eval_freq']):\n",
    "                # evaluate our policy\n",
    "                last_eval = 0\n",
    "                with torch.no_grad():\n",
    "                    print()\n",
    "                    eval_rewards = self.evaluate()\n",
    "                    print(f't:{t}, n:{n}, exp:{exp_schedule.value}, lr:{lr_schedule.value}, total_reward:{total_reward}, total_loss:{total_loss}, eval_rewards:{eval_rewards}')\n",
    "                    eval_rewards_list += [eval_rewards]\n",
    "\n",
    "            if (t > self.config['learning_start']) and self.config['video'] and (last_video > self.config['video_freq']):\n",
    "                # show a video of our policy in action\n",
    "                last_video = 0\n",
    "                with torch.no_grad():\n",
    "                    self.video()\n",
    "\n",
    "                    \n",
    "        return eval_rewards_list\n",
    "\n",
    "                    \n",
    "    def train_step(self, t, replay_buffer, lr):\n",
    "        \"\"\"\n",
    "        Perform training step\n",
    "\n",
    "        Args:\n",
    "            t: (int) nths step\n",
    "            replay_buffer: buffer for sampling\n",
    "            lr: (float) learning rate\n",
    "        \"\"\"\n",
    "        loss = 0\n",
    "\n",
    "        # perform training step\n",
    "        if (t > self.config['learning_start'] and t % self.config['learning_freq'] == 0):\n",
    "            loss = self.update_step(t, replay_buffer, lr)\n",
    "\n",
    "        # occasionaly update target network with q network\n",
    "        if t % self.config['target_update_freq'] == 0:\n",
    "            self.update_target_params()\n",
    "\n",
    "        return loss\n",
    "      \n",
    "    def update_step(self, t, replay_buffer, lr):\n",
    "        \"\"\"\n",
    "        Performs an update of parameters by sampling from replay_buffer\n",
    "\n",
    "        Args:\n",
    "            t: number of iteration (episode and move)\n",
    "            replay_buffer: ReplayBuffer instance .sample() gives batches\n",
    "            lr: (float) learning rate\n",
    "        Returns:\n",
    "            loss\n",
    "        \"\"\"\n",
    "\n",
    "        # s and next_s are (B, obs_dim)\n",
    "        # everything else is (B,)\n",
    "        s, a, r, next_s, done_mask = replay_buffer.sample(\n",
    "            self.config['batch_size'])\n",
    "\n",
    "        s = torch.from_numpy(s).float().to(self.config['device'])\n",
    "        a = torch.from_numpy(a).long().to(self.config['device'])\n",
    "        r = torch.from_numpy(r).float().to(self.config['device'])\n",
    "        next_s = torch.from_numpy(next_s).float().to(self.config['device'])\n",
    "        done_mask = torch.from_numpy(done_mask).float().to(self.config['device'])\n",
    "\n",
    "        B, *_ = s.shape\n",
    "        ##############################################################################\n",
    "        # TODO: Calculate the loss. Instead of the squared loss, we specify above, \n",
    "        # use self.loss(pred, target), where:\n",
    "        #     pred = Q value for (s,a)\n",
    "        #     target = r if the episode is done (use done_mask)\n",
    "        #            = r + gamma * max_a'[Q_target(s', a')]\n",
    "        # Hint: use target.detach() to detach target from the computational graph\n",
    "        # We need to do this because target should be fixed, and we do not want\n",
    "        # gradients to flow through target.\n",
    "        ##############################################################################\n",
    "        # Replace \"pass\" statement with your code \n",
    "        pass\n",
    "        # END OF YOUR CODE\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_(self.q_net.parameters(), self.config['grad_clip_value'])\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss\n",
    "      \n",
    "    def evaluate(self, env=None, num_episodes=None):\n",
    "        \"\"\"\n",
    "        Evaluation with same procedure as the training\n",
    "        \"\"\"\n",
    "        # log our activity only if default call\n",
    "        if num_episodes is None:\n",
    "            print(\"Evaluating...\")\n",
    "\n",
    "        # arguments defaults\n",
    "        if num_episodes is None:\n",
    "            num_episodes = self.config['num_episodes_test']\n",
    "\n",
    "        if env is None:\n",
    "            env = self.env\n",
    "\n",
    "        replay_buffer = ReplayBuffer(self.config['buffer_size'], self.config['frame_history_len'])\n",
    "        rewards = []\n",
    "\n",
    "        for i in range(num_episodes):\n",
    "            total_reward = 0\n",
    "            state = env.reset()\n",
    "            while True:\n",
    "                # store last state in buffer\n",
    "                idx = replay_buffer.store_frame(state)\n",
    "                q_input = replay_buffer.encode_recent_observation()\n",
    "\n",
    "                # during evaluation, we always use best action\n",
    "                action = self.get_best_action(q_input)[0]\n",
    "\n",
    "                # perform action in env\n",
    "                new_state, reward, done, info = env.step(action)\n",
    "\n",
    "                # store in replay memory\n",
    "                replay_buffer.store_effect(idx, action, reward, done)\n",
    "                state = new_state\n",
    "\n",
    "                # count reward\n",
    "                total_reward += reward\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            # updates to perform at the end of an episode\n",
    "            rewards.append(total_reward)\n",
    "\n",
    "        avg_reward = np.mean(rewards)\n",
    "        sigma_reward = np.sqrt(np.var(rewards) / len(rewards))\n",
    "\n",
    "        if num_episodes > 1:\n",
    "            msg = \"Average reward: {:04.2f} +/- {:04.2f}\".format(avg_reward, sigma_reward)\n",
    "            print(msg)\n",
    "\n",
    "        return avg_reward\n",
    "      \n",
    "    def video(self):\n",
    "        \"\"\"\n",
    "        Re create an env and show a video for one episode\n",
    "        \"\"\"\n",
    "        print(\"Showing video...\")\n",
    "        def show_video():\n",
    "          mp4list = glob.glob('video/*.mp4')\n",
    "          if len(mp4list) > 0:\n",
    "            mp4 = mp4list[0]\n",
    "            video = io.open(mp4, 'r+b').read()\n",
    "            encoded = base64.b64encode(video)\n",
    "            ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                        loop controls style=\"height: 400px;\">\n",
    "                        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "                     </video>'''.format(encoded.decode('ascii'))))\n",
    "          else: \n",
    "            print(\"Could not find video\")\n",
    "\n",
    "\n",
    "        def wrap_env(env):\n",
    "#           env = Monitor(env, './video', force=True)\n",
    "#           env = RecordVideo(env, './video')  \n",
    "          env = RecordEpisodeStatistics(env)\n",
    "          return env\n",
    "        \n",
    "        env = gym.make(self.config['env_name'])\n",
    "        env = wrap_env(env)\n",
    "        env = MaxAndSkipEnv(env, skip=self.config['skip_frame'])\n",
    "        env = PreproWrapper(env, prepro=greyscale, shape=(80, 80, 1))\n",
    "        self.evaluate(env, 1)\n",
    "        env.close()\n",
    "        show_video()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN on Test Env\n",
    "\n",
    "We'll first test our implementation on the test environment we described above. This should run in less than a minute so it is useful for debugging purposes. You should see a reward around the optimal maximum reward you determined earlier in the assignment. Don't worry if the reward on the final iteration is not equal to the optimal maximum reward as long as the optimal reward is reached at some point during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(env, config):\n",
    "    env.seed(config['seed'])\n",
    "    env.action_space.seed(config['seed'])\n",
    "    \n",
    "    lr_schedule  = LinearSchedule(config['lr_begin'], config['lr_end'],\n",
    "            config['lr_nsteps'])\n",
    "    exp_schedule = LinearExploration(env, config['eps_begin'],\n",
    "            config['eps_end'], config['eps_nsteps'])\n",
    "    dqn = DQN(env, config)\n",
    "    \n",
    "    if config['video']:\n",
    "        # show initial video\n",
    "        dqn.video()\n",
    "        \n",
    "    rewards = dqn.train(lr_schedule, exp_schedule)\n",
    "    \n",
    "    if config['video']:\n",
    "        # show final video\n",
    "        dqn.video()\n",
    "        \n",
    "    print('Done!')\n",
    "    return dqn, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_env():\n",
    "    config = {\n",
    "        'env_name': 'EnvTest',\n",
    "        'exp_name': 'dqn',\n",
    "        'seed': 1,\n",
    " \n",
    "        'num_episodes_test': 20,\n",
    "        'eval_freq': 100,\n",
    "        'video': False,\n",
    "        'video_freq': 0,\n",
    "      \n",
    "        'batch_size': 32,\n",
    "        'learning_start': 200, # lunar\n",
    "        'learning_freq': 4,\n",
    "        'frame_history_len': 4,\n",
    "\n",
    "        'target_update_freq': 500,\n",
    "        'nsteps_train': 2000,\n",
    "        'buffer_size': 500,\n",
    "\n",
    "        'lr_begin': 0.00025,\n",
    "        'lr_end': 0.0001,\n",
    "        'lr_nsteps': 2000//2,\n",
    "\n",
    "        'eps_begin': 1,\n",
    "        'eps_end': 0.01,\n",
    "        'eps_nsteps': 2000//2,\n",
    "        'gamma': 0.99,\n",
    "\n",
    "        'grad_clip_value': 10,\n",
    "      \n",
    "        'obs_dtype': np.uint8,\n",
    "        'q_net_func': create_q_network,\n",
    "      \n",
    "        'device': device,\n",
    "    }\n",
    "    \n",
    "    torch.manual_seed(config['seed'])\n",
    "    np.random.seed(config['seed'])\n",
    "    random.seed(config['seed'])\n",
    "    \n",
    "    env = EnvTest(shape=(80, 80, 3))\n",
    "    dqn_agent, rewards = run(env, config)\n",
    "    return dqn_agent, rewards, env\n",
    "test_env_dqn_agent, test_env_rewards, test_env_env = run_test_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the rewards curve for the test environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rewards(rewards):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    plt.plot(rewards)\n",
    "    plt.ylabel('Reward')\n",
    "    plt.xlabel('Step')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rewards(test_env_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN on Atari Pong\n",
    "\n",
    "Before running DQN on Pong, we need to define a few wrappers for the Gym environment. The PreproWrapper applies some preprocessing to the images, resizing them to 80x80 and making them greyscale. The MaxAndSkipEnv modifies our environment to take the max reward over a certain number of frames. This helps to stabilize training. We've implemented these for you and you do not need to modify them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greyscale(state):\n",
    "    \"\"\"\n",
    "    Preprocess state (210, 160, 3) image into\n",
    "    a (80, 80, 1) image in grey scale\n",
    "    \"\"\"\n",
    "    state = np.reshape(state, [210, 160, 3]).astype(np.float32)\n",
    "\n",
    "    # grey scale\n",
    "    state = state[:, :, 0] * 0.299 + state[:, :, 1] * 0.587 + state[:, :, 2] * 0.114\n",
    "\n",
    "    # karpathy\n",
    "    state = state[35:195]  # crop\n",
    "    state = state[::2,::2] # downsample by factor of 2\n",
    "\n",
    "    state = state[:, :, np.newaxis]\n",
    "\n",
    "    return state.astype(np.uint8)\n",
    "\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Wrapper from Berkeley's Assignment\n",
    "    Takes a max pool over the last n states\n",
    "    \"\"\"\n",
    "    def __init__(self, env=None, skip=4):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super(MaxAndSkipEnv, self).__init__(env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = deque(maxlen=2)\n",
    "        self._skip       = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            self._obs_buffer.append(obs)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Clear past frame buffer and init. to first obs. from inner env.\"\"\"\n",
    "        self._obs_buffer.clear()\n",
    "        obs = self.env.reset()\n",
    "        self._obs_buffer.append(obs)\n",
    "        return obs\n",
    "\n",
    "class PreproWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Wrapper for Pong to apply preprocessing\n",
    "    Stores the state into variable self.obs\n",
    "    \"\"\"\n",
    "    def __init__(self, env, prepro, shape, high=255):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            env: (gym env)\n",
    "            prepro: (function) to apply to a state for preprocessing\n",
    "            shape: (list) shape of obs after prepro\n",
    "            grey_scale: (bool) if True, assume grey scale, else black and white\n",
    "            high: (int) max value of state after prepro\n",
    "        \"\"\"\n",
    "        super(PreproWrapper, self).__init__(env)\n",
    "        self.prepro = prepro\n",
    "        self.observation_space = spaces.Box(low=0, high=high, shape=shape, dtype=np.uint8)\n",
    "        self.high = high\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Overwrites _step function from environment to apply preprocess\n",
    "        \"\"\"\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.obs = self.prepro(obs)\n",
    "        return self.obs, reward, done, info\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        self.obs = self.prepro(obs)\n",
    "        return self.obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to run Pong. This may take a few hours (~4 hours for our reference solution) to run, but you should start seeing improvements after an hour (ie. you will start seeing Average rewards > -21). If not, then there may be something wrong with your implementation.\n",
    "\n",
    "The maximum reward here is 21 and you should see that as the Average reward at some point during training. Again, don't worry if the reward at your last iteration is not 21, as long as you hit 21 at some point during training.\n",
    "\n",
    "In the video we show during training, the agent is the green paddle on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pong():\n",
    "    config = {\n",
    "        'env_name': 'PongNoFrameskip-v4',\n",
    "        'exp_name': 'dqn',\n",
    "        'seed': 1,\n",
    "      \n",
    "        'num_episodes_test': 20,\n",
    "        'eval_freq': 10000,\n",
    "        'video': True,\n",
    "        'video_freq': 10000,\n",
    "      \n",
    "        'batch_size': 32,\n",
    "        'learning_start': 50000,\n",
    "        'learning_freq': 4,\n",
    "        'frame_history_len': 4,\n",
    "        'skip_frame': 4,\n",
    "      \n",
    "        'target_update_freq': 10000,\n",
    "        'nsteps_train': 2000000,\n",
    "        'buffer_size': 1000000,\n",
    "      \n",
    "        'lr_begin': 0.00025,\n",
    "        'lr_end': 0.00005,\n",
    "        'lr_nsteps': 2000000//2,\n",
    "\n",
    "        'eps_begin': 1,\n",
    "        'eps_end': 0.1,\n",
    "        'eps_nsteps': 2000000 * 0.1,\n",
    "        'gamma': 0.99, # atari\n",
    "      \n",
    "        'grad_clip_value': 10,\n",
    "      \n",
    "        'obs_dtype': np.uint8,\n",
    "        'q_net_func': create_q_network,\n",
    "      \n",
    "        'device': device\n",
    "    }\n",
    "    \n",
    "    env = gym.make(config['env_name'])\n",
    "    env = MaxAndSkipEnv(env, skip=config['skip_frame'])\n",
    "    env = PreproWrapper(env, prepro=greyscale, shape=(84, 84, 1))\n",
    "    dqn_agent, rewards = run(env, config)\n",
    "    return dqn_agent, rewards, env\n",
    "    \n",
    "pong_dqn_agent, pong_rewards, pong_env = run_pong()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the rewards curve for the pong:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rewards(pong_rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
